{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the assignment, you need to do the following steps :\n",
    "\n",
    "1. Read the MovieLens dataset from a file (ratings.csv & movies.csv) instead of loading it directly with using the load_builtin method. For more informations, check the Surprise Dataset module documentation.\n",
    "\n",
    "2. Create 2 model pipelines :\n",
    "\n",
    "1st pipeline : Load data, Train test split, model training, prediction, evaluation.\n",
    "\n",
    "2nd pipeline : Load data, cross validation.\n",
    "\n",
    "3. Benchmark the User based and item based collaborative filtering models using the cosine and pearson correlation similarity metrics. In this step you need to use the data loaded in the 1st step.\n",
    "\n",
    "**Notebook :**\n",
    "\n",
    "Your notebook should be leasable, well organized and commented. It should contain 3 seperate parts :\n",
    "\n",
    "- Data loading\n",
    "- Model pipelines\n",
    "- Model benchmarking\n",
    "\n",
    "\n",
    "**Submission :**\n",
    "\n",
    "The submission deadline is the 20 / 01 @ 17:42.\n",
    "\n",
    "You need to push your code in a github repository and to send the link in the assignment tab.\n",
    "\n",
    "Your repository hierarchy should be the same as the hierarchy used during the practical work (for more information check the shared github repository https://github.com/bachtn/recommender_system_practical_work_students)\n",
    "\n",
    "**NB :** during the next session, I will verify that you are using a separate environment for the practical work. If not you will get a penalty on the practical work grade.\n",
    "\n",
    "If you have any questions, you need to post them in the Discussions channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "DATA_DIR = Path('../data/movielens/ml-latest')\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "ratings = Dataset.load_from_file(DATA_DIR / 'ratings.csv', reader=reader)\n",
    "\n",
    "\n",
    "movies = pd.read_csv(DATA_DIR / 'movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data : For this we will use ml-latest-small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/movielens/ml-latest-small')\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "\n",
    "ratings_small = Dataset.load_from_file(DATA_DIR / 'ratings.csv', reader=reader)\n",
    "\n",
    "data = ratings_small\n",
    "\n",
    "movies_small = pd.read_csv(DATA_DIR / 'movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 8928)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train.n_users, train.n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training user based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# users based and cosine similarity\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True  # compute  similarities between users\n",
    "               }\n",
    "algo_users_cos = KNNBasic(sim_options=sim_options)\n",
    "model_users_cos = algo_users_cos.fit(train)\n",
    "# users based and pearson correlation\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'shrinkage': 0  # no shrinkage\n",
    "               }\n",
    "algo_users_pears = KNNBasic(sim_options=sim_options)\n",
    "model_users_pears = algo_users_pears.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training item based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# item based and cosine similarity\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # compute  similarities between items\n",
    "               }\n",
    "algo_items_cos = KNNBasic(sim_options=sim_options)\n",
    "model_items_cos = algo_items_cos.fit(train)\n",
    "\n",
    "# user based and pearson correlation\n",
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'shrinkage': 0  # no shrinkage\n",
    "               }\n",
    "algo_items_pears = KNNBasic(sim_options=sim_options)\n",
    "model_items_pears = algo_items_pears.fit(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_users_cos = model_users_cos.test(test)\n",
    "predictions_users_pears = model_users_pears.test(test)\n",
    "predictions_items_cos = model_items_cos.test(test)\n",
    "predictions_items_pears = model_items_pears.test(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9806\n",
      "RMSE: 0.9900\n",
      "RMSE: 0.9800\n",
      "RMSE: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9900277794148814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions=predictions_users_cos)\n",
    "accuracy.rmse(predictions=predictions_users_pears)\n",
    "accuracy.rmse(predictions=predictions_items_cos)\n",
    "accuracy.rmse(predictions=predictions_items_pears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "# Load Data\n",
    "data = data\n",
    "# Cross Validation\n",
    "cv_users_cos = cross_validate(model_users_cos, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)\n",
    "cv_users_pears = cross_validate(model_users_pears, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)\n",
    "cv_items_cos = cross_validate(model_items_cos, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)\n",
    "cv_items_pears = cross_validate(model_items_pears, data, measures=['RMSE', 'MAE'], cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data:  for this we will use ml-latest loaded in question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ratings # This was loaded in the first part ( ml-latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us use GridSearchCV to implement the different model, the cross validation and finally to perfom the benchmarking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'sim_options': {'name': ['pearson_baseline', 'cosine'],\n",
    "                               'user_based': [False, True]}\n",
    "              }\n",
    "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse', 'mae'], cv=5)\n",
    "\n",
    "gs.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still processing................... !! the data is really huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(gs.cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
